{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización a Nivel de Palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización a nivel de palabras:\n",
      "['La', 'tokenización', 'de', 'palabras', 'es', 'una', 'tarea', 'importante', 'en', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "def word_tokenizer(text):\n",
    "    # Lista de caracteres de puntuación comunes\n",
    "    punctuation = ['.', ',', ';', ':', '!', '?', '\"', \"'\", '(', ')', '[', ']', '{', '}']\n",
    "\n",
    "    # Inicializar una lista para almacenar los tokens\n",
    "    tokens = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    # Iterar sobre cada carácter en el texto\n",
    "    for char in text:\n",
    "        # Si el carácter es una letra o un número, agregalo a la palabra actual\n",
    "        if char.isalnum():\n",
    "            current_word += char\n",
    "        # Si el carácter es un espacio en blanco o un carácter de puntuación, finaliza la palabra actual\n",
    "        elif char.isspace() or char in punctuation:\n",
    "            # Si la palabra actual no está vacía, agrégala a la lista de tokens\n",
    "            if current_word:\n",
    "                tokens.append(current_word)\n",
    "                current_word = \"\"\n",
    "        # Si el carácter es otro tipo de carácter, omítelo\n",
    "\n",
    "    # Agregar la última palabra si no está vacía\n",
    "    if current_word:\n",
    "        tokens.append(current_word)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Frase de ejemplo\n",
    "sentence = \"La tokenización de palabras es una tarea importante en NLP.\"\n",
    "\n",
    "# Tokenización a nivel de palabras\n",
    "tokens = word_tokenizer(sentence)\n",
    "\n",
    "print(\"Tokenización a nivel de palabras:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"La tokenización de palabras es una tarea importante en NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La',\n",
       " 'tokenización',\n",
       " 'de',\n",
       " 'palabras',\n",
       " 'es',\n",
       " 'una',\n",
       " 'tarea',\n",
       " 'importante',\n",
       " 'en',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/erika/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización a nivel de palabras:\n",
      "['La', 'tokenización', 'es', 'importante', 'para', 'el', 'Procesamiento', 'del', 'Lenguaje', 'Natural', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Frase de ejemplo\n",
    "sentence = \"La tokenización es importante para el Procesamiento del Lenguaje Natural.\"\n",
    "\n",
    "# Tokenización a nivel de palabras\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(\"Tokenización a nivel de palabras:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización a nivel de caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización a nivel de caracteres:\n",
      "['L', 'a', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 'c', 'i', 'ó', 'n', ' ', 'd', 'e', ' ', 'p', 'a', 'l', 'a', 'b', 'r', 'a', 's', ' ', 'e', 's', ' ', 'u', 'n', 'a', ' ', 't', 'a', 'r', 'e', 'a', ' ', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't', 'e', ' ', 'e', 'n', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "def char_tokenizer(text):\n",
    "    # Inicializar una lista para almacenar los tokens\n",
    "    tokens = []\n",
    "\n",
    "    # Iterar sobre cada carácter en el texto\n",
    "    for char in text:\n",
    "        # Agregar el carácter actual a la lista de tokens\n",
    "        tokens.append(char)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Ejemplo de frase\n",
    "sentence = \"La tokenización de palabras es una tarea importante en NLP.\"\n",
    "\n",
    "# Tokenización a nivel de caracteres\n",
    "tokens = char_tokenizer(sentence)\n",
    "\n",
    "print(\"Tokenización a nivel de caracteres:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización a nivel de caracteres:\n",
      "['L', 'a', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 'c', 'i', 'ó', 'n', ' ', 'a', ' ', 'n', 'i', 'v', 'e', 'l', ' ', 'd', 'e', ' ', 'c', 'a', 'r', 'a', 'c', 't', 'e', 'r', 'e', 's', ' ', 'e', 's', ' ', 'ú', 't', 'i', 'l', ' ', 'p', 'a', 'r', 'a', ' ', 'c', 'i', 'e', 'r', 't', 'a', 's', ' ', 'a', 'p', 'l', 'i', 'c', 'a', 'c', 'i', 'o', 'n', 'e', 's', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# Frase de ejemplo\n",
    "sentence = \"La tokenización a nivel de caracteres es útil para ciertas aplicaciones.\"\n",
    "\n",
    "# Expresión regular para coincidir con cada carácter individual\n",
    "pattern = r\"\\S|\\s\"  # Coincide con cualquier carácter no espaciador (\\S) o espacio en blanco (\\s)\n",
    "\n",
    "# Tokenización a nivel de caracteres\n",
    "tokens = regexp_tokenize(sentence, pattern)\n",
    "\n",
    "print(\"Tokenización a nivel de caracteres:\")\n",
    "print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
